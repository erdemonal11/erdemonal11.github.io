<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Erdem Önal</title>
    <link rel="icon" type="image/png" href="images/icon.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <a href="#main" class="skip-link">Skip to main content</a>
    <header class="header">
        <h1><span class="name-highlight">Erdem Önal</span></h1>
        <div class="intro">
            I am a first year MSc student in <a href="https://www.mines-stetienne.fr/formation/cyber-physical-social-systems-cps2/" class="program-link" target="_blank" rel="noopener noreferrer" aria-label="Computer Science CPS2 AI and IoT program (opens in new tab)">Computer Science (CPS2: AI and IoT)</a> at Université Jean Monnet and Ecole Nationale Supérieure des
            Mines de Saint-Etienne. <br><br>
            I am interested in how <span class="highlight-ml">machine learning</span> can be integrated into <span class="highlight-is">intelligent systems</span>.
        </div>
        <div class="social-links">
            <a href="mailto:erdemonal@outlook.fr" aria-label="Email Erdem via Outlook">
                <i class="fas fa-envelope" aria-hidden="true" title="Email"></i>
            </a>
            <a href="https://linkedin.com/in/e-onal" target="_blank" rel="noopener noreferrer" aria-label="LinkedIn profile (opens in new tab)">
                <i class="fab fa-linkedin" aria-hidden="true" title="LinkedIn"></i>
            </a>
            <a href="https://github.com/erdemonal11" target="_blank" rel="noopener noreferrer" aria-label="GitHub profile (opens in new tab)">
                <i class="fab fa-github" aria-hidden="true" title="GitHub"></i>
            </a>
            <a href="https://orcid.org/0009-0006-6580-3711" target="_blank" rel="noopener noreferrer" aria-label="ORCID profile (opens in new tab)">
                <i class="fa-brands fa-orcid" aria-hidden="true" title="ORCID"></i>
            </a>
            <a href="files/CV_Onal_Erdem.pdf" target="_blank" rel="noopener noreferrer" aria-label="Curriculum Vitae PDF (opens in new tab)" type="application/pdf">
                <i class="fas fa-file" aria-hidden="true" title="CV (PDF)"></i>
            </a>
        </div>
    </header>

    <main id="main">
    <div class="section">
        <h2>Publications</h2>
        <div class="research">
            <div class="research-title">Text Mining-Based Profiling of Chemical Environments in Protein-Ligand Binding Assays Across Analytical Techniques</div>
            <div class="research-authors">Kalaycioğlu, Z., & Önal, E. (2025)</div>
            <div class="research-venue">Chemometrics and Intelligent Laboratory Systems (under review)</div>
            <a href="https://www.sciencedirect.com/journal/chemometrics-and-intelligent-laboratory-systems" class="research-paper-link" target="_blank" rel="noopener noreferrer" aria-label="Chemometrics and Intelligent Laboratory Systems journal (opens in new tab)">Journal Link</a>
        </div>
    </div>

    <div class="section">
        <h2>Research</h2>
        <div class="research">
            <div class="research-title">A Systematic Mapping Study on Green Load<br>Balancing Algorithms for Cloud Data Centers</div>
            <div class="research-authors">Önal, E., Alkhori, F., & Schramm, M.</div>
            <a href="files/SMS-Study.pdf" class="research-paper-link" target="_blank" rel="noopener noreferrer">Read the paper (PDF)</a>
        </div>
    </div>

    <div class="section">
        <h2>Notes</h2>
        <script>
            document.addEventListener('DOMContentLoaded', function () {
                const buttons = document.querySelectorAll('.filter-btn');
                const notes = document.querySelectorAll('.note');

                function setActive(button) {
                    buttons.forEach(b => b.classList.remove('active'));
                    button.classList.add('active');
                }

                function filterNotes(category) {
                    notes.forEach(note => {
                        const categories = (note.getAttribute('data-categories') || '').split(',');
                        const normalized = categories.map(c => c.trim());
                        if (category === 'all' || normalized.includes(category)) {
                            note.style.display = '';
                        } else {
                            note.style.display = 'none';
                        }
                    });
                }

                buttons.forEach(btn => {
                    btn.setAttribute('aria-pressed', btn.classList.contains('active') ? 'true' : 'false');
                    btn.addEventListener('click', () => {
                        setActive(btn);
                        buttons.forEach(b => b.setAttribute('aria-pressed', b === btn ? 'true' : 'false'));
                        filterNotes(btn.getAttribute('data-filter'));
                    });
                });
            });
        </script>
        <div class="note-filters" aria-label="Filter notes by category" role="toolbar">
            <button class="filter-btn active" data-filter="all">All</button>
            <button class="filter-btn" data-filter="machine-learning">Machine Learning</button>
            <button class="filter-btn" data-filter="optimization">Optimization</button>
            <button class="filter-btn" data-filter="mathematical-analysis">Mathematical Analysis</button>
            <button class="filter-btn" data-filter="numerical-methods">Numerical Methods</button>
        </div>
        <div class="note" data-categories="optimization">
            <div class="note-title">Gradient Descent</div>
            <div class="note-description">Gradient descent locates local minima by repeatedly stepping in the direction of the steepest decrease. It uses the formula xₙ₊₁ = xₙ - α∇f(xₙ), where α represents the learning rate. The negative gradient -∇f(x) indicates the direction of the most rapid drop in the function's value. Selecting the learning rate involves balancing how fast you converge and keeping things stable. If the rate is too small, convergence is slow. If the learning rate is too big, you might overshoot or diverge.</div>
        </div>
        <div class="note" data-categories="mathematical-analysis">
            <div class="note-title">Hessian Matrix</div>
            <div class="note-description">The Hessian matrix, H, shows the local curvature of functions f(x₁, x₂, ..., xₙ). It's an n×n matrix where the entry (i,j) is ∂²f/∂xᵢ∂xⱼ. This matrix is symmetric if the second derivatives are continuous. In optimization, the Hessian at critical points tells you if the function has a local minimum (positive definite), a local maximum (negative definite), or a saddle point (indefinite).</div>
        </div>
        <div class="note" data-categories="mathematical-analysis">
            <div class="note-title">Jacobian Matrix</div>
            <div class="note-description">The Jacobian matrix, J, is an m×n matrix that contains all the first order partial derivatives of a vector function f: ℝⁿ → ℝᵐ. The element in the (i,j) position is ∂fᵢ/∂xⱼ. The Jacobian gives you the best linear approximation of the function around a certain point. For square matrices, the Jacobian determinant lets you change variables in multiple integrals. If the determinant at a point isn't zero, the Inverse Function Theorem says the function can be locally inverted and has a continuously differentiable inverse.</div>
        </div>
        <div class="note" data-categories="machine-learning">
            <div class="note-title">K-Nearest Neighbors</div>
            <div class="note-description">K-Nearest Neighbors is a lazy learning method that makes predictions based on the K closest neighbors to a point. For classification, it uses a majority vote. For regression, it averages the values of the neighbors. The method measures distances between the query and training points, and then picks the K closest. Choosing K means finding a balance because a small K can be too sensitive to noise, but a big K makes boundaries smoother but might underfit the data. Odd K values are better in binary classification to prevent ties.</div>
        </div>
        <div class="note" data-categories="optimization">
            <div class="note-title">Lagrange Multipliers</div>
            <div class="note-description">Lagrange multipliers are used to solve constrained optimization problems. They minimize f(x) subject to g(x)=0 by creating the Lagrangian L(x,λ)=f(x)−λg(x). To find the solution, you set all partial derivatives to zero, which gives you ∇f(x)=λ∇g(x) and g(x)=0. The multiplier λ shows how the optimal value changes based on the constraint.</div>
        </div>
        <div class="note" data-categories="machine-learning">
            <div class="note-title">Logistic Regression</div>
            <div class="note-description">Logistic regression is a model used to classify things in machine learning. It starts by adding up the input features in a weighted way, plus a bias. This is written as z = w⋅x + b. Then, it uses the sigmoid function, which turns any number into a value between 0 and 1 using the formula g(z) = 1/(1 + e⁻ᶻ). The result can be seen as the likelihood that the input fits into a certain class. This makes logistic regression helpful for sorting inputs into categories.</div>
        </div>
        <div class="note" data-categories="numerical-methods">
            <div class="note-title">Newton Raphson Method</div>
            <div class="note-description">The Newton Raphson method finds roots of the equation f(x)=0 using the iterative formula xₙ₊₁ = xₙ - f(xₙ)/f'(xₙ). Starting with a guess, x₀, each step uses the tangent line at the current point to guess where the function crosses the x axis. This method works when the initial guess is close enough to the root, f'(x)≠0 near the root, and the function changes smoothly. Common ways to check if it have converged are |xₙ₊₁ - xₙ| &lt; ε or |f(xₙ)| &lt; ε, where ε is a small value.</div>
        </div>
        <div class="note" data-categories="machine-learning">
            <div class="note-title">ReLU (Rectified Linear Unit)</div>
            <div class="note-description">ReLU is expressed as f(x) = max(0, x). It lets positive inputs go through as they are, but it outputs zero for negative inputs. It is good for calculations and helps with the vanishing gradient issue since its derivative is 1 for positive inputs and 0 for negative inputs. By setting negative values to zero, ReLU adds sparsity, which can speed up networks and reduce overfitting.</div>
        </div>
        <div class="note" data-categories="machine-learning">
            <div class="note-title">Regularization</div>
            <div class="note-description">Regularization stops overfitting by adding a penalty to the cost function that discourages big parameter values. For both linear and logistic regression, the regularized cost function is J(w,b) = original_cost + (λ/2m)∑wⱼ². The regularization parameter λ controls how to balance fitting the training data and keeping parameters small. In gradient descent, this adds λ/m·wⱼ to the derivative of wⱼ, which shrinks parameters by multiplying them by (1 - αλ/m) each time. Note that the bias parameter b isn't regularized, just the weight parameters w are penalized to prevent overfitting.</div>
        </div>
        <div class="note" data-categories="machine-learning">
            <div class="note-title">Softmax</div>
            <div class="note-description">Softmax changes a list of real numbers into a probability distribution, σ(zᵢ) = eᶻⁱ / ∑ⱼeᶻʲ. Each output is between 0 and 1, and they all add up to 1. This makes it good for multiclass classification in the output layer. Softmax makes differences between inputs bigger so that larger values get higher probabilities, and smaller values get lower ones. The function can be differentiated everywhere, so it works well for backpropagation.</div>
        </div>
    </div>

    <div class="badges-container">
        <div class="badge-group">
            <a href="https://www.w3.org/WAI/WCAG2AA-Conformance"
               title="Explanation of WCAG 2 Level AA conformance"
               target="_blank" rel="noopener noreferrer"
               aria-label="WCAG 2.2 Level AA conformance (opens in new tab)">
                <img height="32" width="88"
                     src="https://www.w3.org/WAI/WCAG22/wcag2.2AA"
                     alt="Level AA conformance, W3C WAI Web Content Accessibility Guidelines 2.2">
            </a>
        </div>
        <div class="badge-group">
            <a href="https://bff.ecoindex.fr/redirect/?url=https://erdemonal11.github.io/" 
               target="_blank" rel="noopener noreferrer"
               title="EcoIndex environmental impact score for this website"
               aria-label="Ecoindex environmental impact badge (opens in new tab)">
                <img src="https://bff.ecoindex.fr/badge/?theme=light&url=https://erdemonal11.github.io/" 
                     alt="EcoIndex Badge" height="32"/>
            </a>
        </div>
    </div>

    </main>

</body>

</html>